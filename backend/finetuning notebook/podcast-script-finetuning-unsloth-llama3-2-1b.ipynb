{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12375789,"sourceType":"datasetVersion","datasetId":7803414}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install unsloth\n# Also get the latest nightly Unsloth!\n!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git@nightly git+https://github.com/unslothai/unsloth-zoo.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T21:50:22.896922Z","iopub.execute_input":"2025-07-04T21:50:22.897079Z","iopub.status.idle":"2025-07-04T21:53:51.941481Z","shell.execute_reply.started":"2025-07-04T21:50:22.897065Z","shell.execute_reply":"2025-07-04T21:53:51.940447Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nfrom unsloth.chat_templates import get_chat_template\nfrom datasets import Dataset\nimport torch\nimport json\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n    max_seq_length = 1024,  # Reduced for stability\n    dtype = None,\n    load_in_4bit = True,\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T21:53:55.513259Z","iopub.execute_input":"2025-07-04T21:53:55.513520Z","iopub.status.idle":"2025-07-04T21:54:41.061886Z","shell.execute_reply.started":"2025-07-04T21:53:55.513492Z","shell.execute_reply":"2025-07-04T21:54:41.061096Z"}},"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-07-04 21:54:09.230639: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751666049.450540      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751666049.508137      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.6.12: Fast Llama patching. Transformers: 4.51.3.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.03G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c75ab301297545eca6e56b4da55e2bb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe6f0419feff4461b1c5329bb3cef47b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"726f76881cb5459d8e254a67cdb65c7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13e105fa88034881a9e926d373916543"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26cd380a240f4e258e481235fbedf07e"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 3407,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T21:54:43.643214Z","iopub.execute_input":"2025-07-04T21:54:43.643976Z","iopub.status.idle":"2025-07-04T21:54:49.898566Z","shell.execute_reply.started":"2025-07-04T21:54:43.643951Z","shell.execute_reply":"2025-07-04T21:54:49.898000Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.6.12 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"tokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"llama-3.1\",\n    mapping = {\"role\" : \"role\", \"content\" : \"content\", \"user\" : \"user\", \"assistant\" : \"assistant\"},\n    map_eos_token = True,\n)\n\ndef formatting_prompts_func(examples):\n    convos = examples[\"messages\"]\n    texts = []\n    for convo in convos:\n        text = tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False)\n        texts.append(text)\n    return { \"text\" : texts, }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T21:54:50.283042Z","iopub.execute_input":"2025-07-04T21:54:50.283903Z","iopub.status.idle":"2025-07-04T21:54:50.289016Z","shell.execute_reply.started":"2025-07-04T21:54:50.283875Z","shell.execute_reply":"2025-07-04T21:54:50.288307Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"with open(\"/kaggle/input/pro-pod/professional_podcast_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T21:54:53.547658Z","iopub.execute_input":"2025-07-04T21:54:53.548019Z","iopub.status.idle":"2025-07-04T21:54:53.559368Z","shell.execute_reply.started":"2025-07-04T21:54:53.547995Z","shell.execute_reply":"2025-07-04T21:54:53.558638Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Use subset for initial training\ndataset = Dataset.from_list(data[:16])  # Start with 16 conversations\ndataset = dataset.map(formatting_prompts_func, batched = True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T21:54:55.492490Z","iopub.execute_input":"2025-07-04T21:54:55.492984Z","iopub.status.idle":"2025-07-04T21:54:55.738128Z","shell.execute_reply.started":"2025-07-04T21:54:55.492962Z","shell.execute_reply":"2025-07-04T21:54:55.737336Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/16 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2083eaa6365249fd9ff83558845a2327"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = 1024,\n    dataset_num_proc = 1,  # Single process for stability\n    packing = False,\n    args = TrainingArguments(\n        per_device_train_batch_size = 1,  # Very conservative\n        gradient_accumulation_steps = 16, # Maintain effective batch size\n        warmup_steps = 5,\n        max_steps = 50,  # Shorter training for testing\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 2,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to = \"none\",\n        save_steps = 25,\n        save_total_limit = 2,\n        dataloader_drop_last = True,\n        remove_unused_columns = False,\n        # Additional stability settings\n        dataloader_num_workers = 0,\n        eval_strategy = \"no\",\n        save_safetensors = True,\n    ),\n)\n\ntrainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T21:54:59.442761Z","iopub.execute_input":"2025-07-04T21:54:59.443325Z","iopub.status.idle":"2025-07-04T21:58:09.309626Z","shell.execute_reply.started":"2025-07-04T21:54:59.443300Z","shell.execute_reply":"2025-07-04T21:58:09.308881Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"]:   0%|          | 0/16 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1a1214026fe4e5695af4a7de122bd34"}},"metadata":{}},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 16 | Num Epochs = 50 | Total steps = 50\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 16\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 16 x 1) = 32\n \"-____-\"     Trainable parameters = 11,272,192 of 1,247,086,592 (0.90% trained)\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 02:59, Epoch 50/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2</td>\n      <td>2.760100</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.597700</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.166400</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.715100</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.477600</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.242200</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.034000</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.870100</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.734200</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.612500</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.513800</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.428800</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.359600</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.298800</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.249200</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.211900</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.183800</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.162500</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.147300</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.136100</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.127800</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.121900</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.117700</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.114900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.113300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"FastLanguageModel.for_inference(model)\n\ntest_prompts = [\n    \"Welcome to our podcast! Today we're discussing quantum computing with Dr. Sarah Chen. What initially drew you to quantum computing?\",\n    \"What's the most exciting breakthrough in artificial intelligence recently?\", \n    \"How do you see renewable energy transforming our future?\"\n]\n\nfor i, prompt in enumerate(test_prompts, 1):\n    print(f\"\\nðŸ”¹ TEST {i}:\")\n    print(f\"HOST: {prompt}\")\n    \n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    \n    inputs = tokenizer.apply_chat_template(\n        messages,\n        tokenize = True,\n        add_generation_prompt = True,\n        return_tensors = \"pt\",\n    ).to(\"cuda\")\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids = inputs, \n            max_new_tokens = 100, \n            use_cache = True,\n            temperature = 0.8,\n            do_sample = True,\n            pad_token_id = tokenizer.eos_token_id,\n            eos_token_id = tokenizer.eos_token_id,\n        )\n    \n    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n    print(f\"GUEST: {response}\")\n    print(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T22:04:45.641036Z","iopub.execute_input":"2025-07-04T22:04:45.641306Z","iopub.status.idle":"2025-07-04T22:04:47.642262Z","shell.execute_reply.started":"2025-07-04T22:04:45.641287Z","shell.execute_reply":"2025-07-04T22:04:47.641700Z"}},"outputs":[{"name":"stdout","text":"\nðŸ”¹ TEST 1:\nHOST: Welcome to our podcast! Today we're discussing quantum computing with Dr. Sarah Chen. What initially drew you to quantum computing?\nGUEST: Thank you for having me! I'm excited to discuss quantum computing and share what's happening at the cutting edge.\n--------------------------------------------------\n\nðŸ”¹ TEST 2:\nHOST: What's the most exciting breakthrough in artificial intelligence recently?\nGUEST: Recent developments in artificial intelligence suggest significant advancements, particularly in neural networks and reasoning capabilities.\n--------------------------------------------------\n\nðŸ”¹ TEST 3:\nHOST: How do you see renewable energy transforming our future?\nGUEST: Renewable energy is transforming our future by providing clean, sustainable, and infinite energy sources.\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"model.save_pretrained(\"podcast_model_final\")\ntokenizer.save_pretrained(\"podcast_model_final\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T22:05:10.256121Z","iopub.execute_input":"2025-07-04T22:05:10.256597Z","iopub.status.idle":"2025-07-04T22:05:10.855751Z","shell.execute_reply.started":"2025-07-04T22:05:10.256575Z","shell.execute_reply":"2025-07-04T22:05:10.854970Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"('podcast_model_final/tokenizer_config.json',\n 'podcast_model_final/special_tokens_map.json',\n 'podcast_model_final/tokenizer.json')"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# First, install huggingface_hub if not already installed\n!pip install huggingface_hub\n\n# Login to Hugging Face (you'll need to enter your token)\nfrom huggingface_hub import login\nlogin()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T21:27:12.286707Z","iopub.execute_input":"2025-07-04T21:27:12.287448Z","iopub.status.idle":"2025-07-04T21:27:15.414753Z","shell.execute_reply.started":"2025-07-04T21:27:12.287426Z","shell.execute_reply":"2025-07-04T21:27:15.413951Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.31.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.4.26)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25ededce67b3407ba49e6c2661a4d9e5"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# Save and push to Hugging Face Hub\nmodel.push_to_hub(\n    \"navth/podcast-llama3-2-1b-finetuned\",  # Replace with your desired model name\n    token=True,  # Uses the token from login()\n    private=False,  # Set to True if you want a private repo\n    safe_serialization=True\n)\n\ntokenizer.push_to_hub(\n    \"navth/podcast-llama3-2-1b-finetuned\",  # Same repo name\n    token=True,\n    private=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T21:30:21.837034Z","iopub.execute_input":"2025-07-04T21:30:21.837728Z","iopub.status.idle":"2025-07-04T21:30:26.951147Z","shell.execute_reply.started":"2025-07-04T21:30:21.837704Z","shell.execute_reply":"2025-07-04T21:30:26.950515Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/594 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c693939aefe444198f479c68d935790e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0ff231a60134a4db876d78eb15adcc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/45.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38cd35f31ead448c916f13f37ba8b59e"}},"metadata":{}},{"name":"stdout","text":"Saved model to https://huggingface.co/navth/podcast-llama3-2-1b-finetuned\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ff52ce7833a4d9e93ad49d3cdc3ca02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f63179bd86ea4dfb81ccc83da8533b2e"}},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# Load base model for testing\nfrom unsloth import FastLanguageModel\nimport torch\n\nbase_model, base_tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n    max_seq_length = 1024,\n    dtype = None,\n    load_in_4bit = True,\n)\nFastLanguageModel.for_inference(base_model)\n\ntest_prompts = [\n    \"Welcome to our podcast! Today we're discussing quantum computing with Dr. Sarah Chen. What initially drew you to quantum computing?\",\n    \"What's the most exciting breakthrough in artificial intelligence recently?\", \n    \"How do you see renewable energy transforming our future?\"\n]\n\nprint(\"ðŸ“ BASE MODEL RESPONSES ONLY\\n\" + \"=\"*60)\n\nfor i, prompt in enumerate(test_prompts, 1):\n    print(f\"\\nðŸ”¹ TEST {i}:\")\n    print(f\"HOST: {prompt}\")\n    \n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    \n    inputs = base_tokenizer.apply_chat_template(\n        messages,\n        tokenize = True,\n        add_generation_prompt = True,\n        return_tensors = \"pt\",\n    ).to(\"cuda\")\n    \n    with torch.no_grad():\n        outputs = base_model.generate(\n            input_ids = inputs, \n            max_new_tokens = 100, \n            use_cache = True,\n            temperature = 0.8,\n            do_sample = True,\n            pad_token_id = base_tokenizer.eos_token_id,\n            eos_token_id = base_tokenizer.eos_token_id,\n        )\n    \n    response = base_tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n    print(f\"BASE MODEL: {response}\")\n    print(\"-\" * 60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T22:05:22.206956Z","iopub.execute_input":"2025-07-04T22:05:22.207646Z","iopub.status.idle":"2025-07-04T22:05:33.372999Z","shell.execute_reply.started":"2025-07-04T22:05:22.207615Z","shell.execute_reply":"2025-07-04T22:05:33.372268Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.6.12: Fast Llama patching. Transformers: 4.51.3.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nðŸ“ BASE MODEL RESPONSES ONLY\n============================================================\n\nðŸ”¹ TEST 1:\nHOST: Welcome to our podcast! Today we're discussing quantum computing with Dr. Sarah Chen. What initially drew you to quantum computing?\nBASE MODEL: Thank you for having me on the show. I'm excited to be here.\n\nDr. Sarah Chen: Welcome back to the podcast. I'm Dr. Sarah Chen, and I'm thrilled to be here with our guest, Dr. Rachel Kim.\n\nDr. Sarah Chen: Dr. Rachel, it's great to have you on the show. Can you start by telling us what initially drew you to quantum computing?\n\nDr. Rachel Kim: Well, I've been fascinated by the intersection of physics and\n------------------------------------------------------------\n\nðŸ”¹ TEST 2:\nHOST: What's the most exciting breakthrough in artificial intelligence recently?\nBASE MODEL: There have been several exciting breakthroughs in artificial intelligence (AI) in recent years. Here are a few examples:\n\n1. **AlphaGo**: AlphaGo is a computer program that beat a human world champion in Go, a complex board game. Developed by Google, AlphaGo uses a unique combination of machine learning and symbolic reasoning to analyze vast amounts of data and make decisions at the speed of human thought.\n2. **DeepMind's AlphaStar**: AlphaStar is a AI system developed by DeepMind\n------------------------------------------------------------\n\nðŸ”¹ TEST 3:\nHOST: How do you see renewable energy transforming our future?\nBASE MODEL: Renewable energy is transforming our future in numerous ways, driving significant positive changes across various sectors. Here are some key aspects:\n\n1. **Energy Independence**: Renewable energy sources like solar, wind, and hydroelectric power can reduce our reliance on fossil fuels and lower our carbon footprint, helping us achieve energy independence.\n\n2. **Job Creation and Economic Growth**: The renewable energy sector is creating new job opportunities and driving economic growth, as companies expand their operations and invest in clean technologies.\n\n3. **\n------------------------------------------------------------\n","output_type":"stream"}],"execution_count":12}]}